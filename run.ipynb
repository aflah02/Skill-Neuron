{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade transformers\n",
    "# !pip install -U adapter-transformers -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed:  2696056113\n",
      "['']\n",
      "[33407, 22173]\n",
      "2\n",
      "2022-12-26 12:36:09,489 - INFO - trainer - Namespace(bz=128, data_path='/home/aflah20082/Skill-Neuron/data/founta', device='cuda:0', early_stop=False, epoch=4, eval_every_step=2000, from_pretrained=False, load_backbone='', lr=0.001, model_type='RobertaPrompt', num_labels=2, prompt_size=16, random_seed=0, resume_from='', save_to='/home/aflah20082/Skill-Neuron/founta', task_type='founta', verb=[''])\n",
      "2022-12-26 12:36:09,490 - INFO - trainer - num_steps_per_dataset:\n",
      "2022-12-26 12:36:09,490 - INFO - trainer - 204\n",
      "2022-12-26 12:36:09,490 - INFO - trainer - total_steps:\n",
      "2022-12-26 12:36:09,490 - INFO - trainer - 816\n",
      "2022-12-26 12:36:09,490 - INFO - trainer - num_train_epochs:\n",
      "2022-12-26 12:36:09,490 - INFO - trainer - 4\n",
      "Epoch:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                        | 0/204 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aflah20082/Skill-Neuron/src/train.py\", line 61, in <module>\n",
      "    main()\n",
      "  File \"/home/aflah20082/Skill-Neuron/src/train.py\", line 58, in main\n",
      "    t.train(train, valid, test, num_train_epochs = args.epoch)\n",
      "  File \"/home/aflah20082/Skill-Neuron/src/trainer.py\", line 111, in train\n",
      "    loss = self.criterion(self.mlm_train_step(batch),labels.long())\n",
      "  File \"/home/aflah20082/Skill-Neuron/src/trainer.py\", line 59, in mlm_train_step\n",
      "    outputs = self.model(sentences)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/Skill-Neuron/src/modelzoo.py\", line 140, in forward\n",
      "    attention_mask=final_mask)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1140, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/adapters/context.py\", line 108, in wrapper_func\n",
      "    results = f(self, *args, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 889, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 555, in forward\n",
      "    output_attentions,\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 476, in forward\n",
      "    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/pytorch_utils.py\", line 243, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 487, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 383, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/activations.py\", line 56, in forward\n",
      "    return self.act(input)\n",
      "  File \"/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py\", line 1556, in gelu\n",
      "    return torch._C._nn.gelu(input)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 31.75 GiB total capacity; 6.02 GiB already allocated; 106.50 MiB free; 6.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python /home/aflah20082/Skill-Neuron/src/train.py\\\n",
    "        --task_type founta\\\n",
    "        --data_path /home/aflah20082/Skill-Neuron/data/founta\\\n",
    "        --save_to /home/aflah20082/Skill-Neuron/founta\\\n",
    "        --bz 128\\\n",
    "        --eval_every_step 2000\\\n",
    "        --model_type RobertaPrompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a67492e6edd839247f88539501b6e58f755504339f85783a0bf23372fdc3c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
